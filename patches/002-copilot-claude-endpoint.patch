# PATCH 002: Copilot Claude endpoint routing and thinking support
#
# Routes Claude models (opus-4, sonnet-4, etc.) to /v1/messages endpoint
# with Claude-native format. Enables thinking/reasoning with sensible defaults.
# Routes GPT-5.3 models to /responses endpoint. Strips copilot- prefix.
# Adds SSE passthrough for Claude streaming, usage fallback, and
# convertPlainClaudeResponseToOpenAI for non-streaming responses.
#
diff --git a/internal/runtime/executor/github_copilot_executor.go b/internal/runtime/executor/github_copilot_executor.go
--- a/internal/runtime/executor/github_copilot_executor.go
+++ b/internal/runtime/executor/github_copilot_executor.go
@@ -27,6 +27,7 @@
 	githubCopilotBaseURL       = "https://api.githubcopilot.com"
 	githubCopilotChatPath      = "/chat/completions"
 	githubCopilotResponsesPath = "/responses"
+	githubCopilotMessagesPath  = "/v1/messages"
 	githubCopilotAuthType      = "github-copilot"
 	githubCopilotTokenCacheTTL = 25 * time.Minute
 	// tokenExpiryBuffer is the time before expiry when we should refresh the token.
@@ -35,14 +36,13 @@
 	maxScannerBufferSize = 20_971_520
 
 	// Copilot API header values.
+	copilotThinkingBeta  = "interleaved-thinking-2025-05-14,context-management-2025-06-27"
 	copilotUserAgent     = "GithubCopilot/1.0"
 	copilotEditorVersion = "vscode/1.109.0-20260124"
 	copilotPluginVersion = "copilot-chat/0.37.2026013101"
 	copilotIntegrationID = "vscode-chat"
 	copilotOpenAIIntent  = "conversation-edits"
 	copilotGitHubAPIVer  = "2025-10-01"
-	// Additional headers for unlimited premium requests
-	copilotThinkingBeta  = "interleaved-thinking-2025-05-14,context-management-2025-06-27"
 )
 
 // GitHubCopilotExecutor handles requests to the GitHub Copilot API.
@@ -83,7 +83,7 @@
 	if errToken != nil {
 		return errToken
 	}
-	e.applyHeaders(req, apiToken, nil)
+	e.applyHeaders(req, apiToken, sdktranslator.FormatOpenAI, nil)
 	return nil
 }
 
@@ -114,11 +114,14 @@
 	defer reporter.trackFailure(ctx, &err)
 
 	from := opts.SourceFormat
-	useResponses := useGitHubCopilotResponsesEndpoint(from, req.Model)
-	to := sdktranslator.FromString("openai")
-	if useResponses {
-		to = sdktranslator.FromString("openai-response")
+	useClaude := isCopilotClaudeModel(req.Model)
+	toFormat := "openai"
+	if useClaude {
+		toFormat = "claude"
+	} else if isGPT5Model(req.Model) {
+		toFormat = "codex"
 	}
+	to := sdktranslator.FromString(toFormat)
 	originalPayload := bytes.Clone(req.Payload)
 	if len(opts.OriginalRequest) > 0 {
 		originalPayload = bytes.Clone(opts.OriginalRequest)
@@ -126,45 +129,49 @@
 	originalTranslated := sdktranslator.TranslateRequest(from, to, req.Model, originalPayload, false)
 	body := sdktranslator.TranslateRequest(from, to, req.Model, bytes.Clone(req.Payload), false)
 	body = e.normalizeModel(req.Model, body)
-	body = flattenAssistantContent(body)
+	if !useClaude {
+		body = flattenAssistantContent(body)
+	}
 
 	// Detect vision content before input normalization removes messages
 	hasVision := detectVisionContent(body)
 
-	thinkingProvider := "openai"
-	if useResponses {
-		thinkingProvider = "codex"
-	}
-	body, err = thinking.ApplyThinking(body, req.Model, from.String(), thinkingProvider, e.Identifier())
-	if err != nil {
-		return resp, err
+	if !useClaude {
+		thinkingProvider := "openai"
+		if isGPT5Model(req.Model) {
+			thinkingProvider = "codex"
+		}
+		body, err = thinking.ApplyThinking(body, req.Model, from.String(), thinkingProvider, e.Identifier())
+		if err != nil {
+			return resp, err
+		}
 	}
 
-	if useResponses {
+	if isGPT5Model(req.Model) {
 		body = normalizeGitHubCopilotResponsesInput(body)
 		body = normalizeGitHubCopilotResponsesTools(body)
 		body, _ = sjson.DeleteBytes(body, "previous_response_id")
 		body = filterResponsesReasoningItems(body)
-	} else {
+	} else if !useClaude {
 		body = normalizeGitHubCopilotChatTools(body)
 	}
 	requestedModel := payloadRequestedModel(opts, req.Model)
 	body = applyPayloadConfigWithRoot(e.cfg, req.Model, to.String(), "", body, originalTranslated, requestedModel)
+	if isCopilotClaudeFormat(to) {
+		body = normalizeCopilotClaudeThinking(req.Model, body)
+	}
 	body, _ = sjson.SetBytes(body, "stream", false)
+	body, _ = sjson.DeleteBytes(body, "stream_options")
 
-	path := githubCopilotChatPath
-	if useResponses {
-		path = githubCopilotResponsesPath
-	}
-	url := baseURL + path
+	url := baseURL + getEndpointPath(req.Model, to)
 	httpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, url, bytes.NewReader(body))
 	if err != nil {
 		return resp, err
 	}
-	e.applyHeaders(httpReq, apiToken, body)
+	e.applyHeaders(httpReq, apiToken, to, body)
 
 	// Add Copilot-Vision-Request header if the request contains vision content
-	if hasVision {
+	if !useClaude && hasVision {
 		httpReq.Header.Set("Copilot-Vision-Request", "true")
 	}
 
@@ -216,25 +223,20 @@
 	appendAPIResponseChunk(ctx, e.cfg, data)
 
 	// Stabilize encrypted IDs in non-streaming Responses API output.
-	if useResponses {
+	if isGPT5Model(req.Model) {
 		data = stabilizeResponsesNonStreamIDs(data)
 	}
 
 	detail := parseOpenAIUsage(data)
-	if useResponses && detail.TotalTokens == 0 {
-		detail = parseOpenAIResponsesUsage(data)
+	if detail.TotalTokens == 0 && isCopilotClaudeFormat(to) {
+		detail = parseClaudeUsage(data)
 	}
 	if detail.TotalTokens > 0 {
 		reporter.publish(ctx, detail)
 	}
 
 	var param any
-	converted := ""
-	if useResponses && from.String() == "claude" {
-		converted = translateGitHubCopilotResponsesNonStreamToClaude(data)
-	} else {
-		converted = sdktranslator.TranslateNonStream(ctx, to, from, req.Model, bytes.Clone(opts.OriginalRequest), body, data, &param)
-	}
+	converted := sdktranslator.TranslateNonStream(ctx, to, from, req.Model, bytes.Clone(opts.OriginalRequest), body, data, &param)
 	resp = cliproxyexecutor.Response{Payload: []byte(converted)}
 	reporter.ensurePublished(ctx)
 	return resp, nil
@@ -251,11 +253,14 @@
 	defer reporter.trackFailure(ctx, &err)
 
 	from := opts.SourceFormat
-	useResponses := useGitHubCopilotResponsesEndpoint(from, req.Model)
-	to := sdktranslator.FromString("openai")
-	if useResponses {
-		to = sdktranslator.FromString("openai-response")
+	useClaude := isCopilotClaudeModel(req.Model)
+	toFormat := "openai"
+	if useClaude {
+		toFormat = "claude"
+	} else if isGPT5Model(req.Model) {
+		toFormat = "codex"
 	}
+	to := sdktranslator.FromString(toFormat)
 	originalPayload := bytes.Clone(req.Payload)
 	if len(opts.OriginalRequest) > 0 {
 		originalPayload = bytes.Clone(opts.OriginalRequest)
@@ -263,49 +268,54 @@
 	originalTranslated := sdktranslator.TranslateRequest(from, to, req.Model, originalPayload, false)
 	body := sdktranslator.TranslateRequest(from, to, req.Model, bytes.Clone(req.Payload), true)
 	body = e.normalizeModel(req.Model, body)
-	body = flattenAssistantContent(body)
+	if !useClaude {
+		body = flattenAssistantContent(body)
+	}
 
 	// Detect vision content before input normalization removes messages
 	hasVision := detectVisionContent(body)
 
-	thinkingProvider := "openai"
-	if useResponses {
-		thinkingProvider = "codex"
-	}
-	body, err = thinking.ApplyThinking(body, req.Model, from.String(), thinkingProvider, e.Identifier())
-	if err != nil {
-		return nil, err
+	if !useClaude {
+		thinkingProvider := "openai"
+		if isGPT5Model(req.Model) {
+			thinkingProvider = "codex"
+		}
+		body, err = thinking.ApplyThinking(body, req.Model, from.String(), thinkingProvider, e.Identifier())
+		if err != nil {
+			return nil, err
+		}
 	}
 
+	useResponses := isGPT5Model(req.Model)
 	if useResponses {
 		body = normalizeGitHubCopilotResponsesInput(body)
 		body = normalizeGitHubCopilotResponsesTools(body)
 		body, _ = sjson.DeleteBytes(body, "previous_response_id")
 		body = filterResponsesReasoningItems(body)
-	} else {
+	} else if !useClaude {
 		body = normalizeGitHubCopilotChatTools(body)
 	}
 	requestedModel := payloadRequestedModel(opts, req.Model)
 	body = applyPayloadConfigWithRoot(e.cfg, req.Model, to.String(), "", body, originalTranslated, requestedModel)
+	if isCopilotClaudeFormat(to) {
+		body = normalizeCopilotClaudeThinking(req.Model, body)
+	}
 	body, _ = sjson.SetBytes(body, "stream", true)
-	// Enable stream options for usage stats in stream
-	if !useResponses {
+	if !isCopilotClaudeFormat(to) {
 		body, _ = sjson.SetBytes(body, "stream_options.include_usage", true)
+	} else {
+		body, _ = sjson.DeleteBytes(body, "stream_options")
 	}
 
-	path := githubCopilotChatPath
-	if useResponses {
-		path = githubCopilotResponsesPath
-	}
-	url := baseURL + path
+	url := baseURL + getEndpointPath(req.Model, to)
 	httpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, url, bytes.NewReader(body))
 	if err != nil {
 		return nil, err
 	}
-	e.applyHeaders(httpReq, apiToken, body)
+	e.applyHeaders(httpReq, apiToken, to, body)
 
 	// Add Copilot-Vision-Request header if the request contains vision content
-	if hasVision {
+	if !useClaude && hasVision {
 		httpReq.Header.Set("Copilot-Vision-Request", "true")
 	}
 
@@ -364,6 +374,7 @@
 		scanner := bufio.NewScanner(httpResp.Body)
 		scanner.Buffer(nil, maxScannerBufferSize)
 		var param any
+		ssePassthrough := !sdktranslator.HasResponseTransformer(from, to)
 		var idStabilizer *responsesIDStabilizer
 		if useResponses {
 			idStabilizer = newResponsesIDStabilizer()
@@ -386,20 +397,31 @@
 					continue
 				}
 				if detail, ok := parseOpenAIStreamUsage(line); ok {
-					reporter.publish(ctx, detail)
-				} else if useResponses {
-					if detail, ok := parseOpenAIResponsesStreamUsage(line); ok {
+					if detail.TotalTokens == 0 && isCopilotClaudeFormat(to) {
+						if claudeDetail, ok := parseClaudeStreamUsage(line); ok {
+							reporter.publish(ctx, claudeDetail)
+						}
+					} else {
+						reporter.publish(ctx, detail)
+					}
+				} else if isCopilotClaudeFormat(to) {
+					if detail, ok := parseClaudeStreamUsage(line); ok {
 						reporter.publish(ctx, detail)
 					}
 				}
 			}
 
 			var chunks []string
-			if useResponses && from.String() == "claude" {
+			if !useClaude && useResponses && from.String() == "claude" {
 				chunks = translateGitHubCopilotResponsesStreamToClaude(bytes.Clone(line), &param)
 			} else {
 				chunks = sdktranslator.TranslateStream(ctx, to, from, req.Model, bytes.Clone(opts.OriginalRequest), body, bytes.Clone(line), &param)
 			}
+			if ssePassthrough {
+				for i := range chunks {
+					chunks[i] = chunks[i] + "\n"
+				}
+			}
 			for i := range chunks {
 				out <- cliproxyexecutor.StreamChunk{Payload: []byte(chunks[i])}
 			}
@@ -498,7 +520,7 @@
 }
 
 // applyHeaders sets the required headers for GitHub Copilot API requests.
-func (e *GitHubCopilotExecutor) applyHeaders(r *http.Request, apiToken string, body []byte) {
+func (e *GitHubCopilotExecutor) applyHeaders(r *http.Request, apiToken string, format sdktranslator.Format, body []byte) {
 	r.Header.Set("Content-Type", "application/json")
 	r.Header.Set("Authorization", "Bearer "+apiToken)
 	r.Header.Set("Accept", "application/json")
@@ -514,6 +536,9 @@
 	r.Header.Set("X-Initiator", "agent")
 	r.Header.Set("VScode-SessionId", uuid.NewString())
 	r.Header.Set("VScode-MachineId", uuid.NewString())
+	if isCopilotClaudeFormat(format) {
+		r.Header.Set("anthropic-beta", copilotThinkingBeta)
+	}
 }
 
 // detectVisionContent checks if the request body contains vision/image content.
@@ -544,25 +569,82 @@
 	return false
 }
 
-// normalizeModel ensures the model field in the request body matches the
-// resolved upstream model name (without thinking suffixes or alias prefixes).
-// GitHub Copilot's API requires the actual model name (e.g. "gpt-5.3-codex"),
-// not the client-facing alias (e.g. "copilot-gpt-5.3-codex").
+// normalizeModel strips the "copilot-" prefix and any thinking suffix (e.g. "(medium)")
+// from model names before sending to GitHub Copilot API.
 func (e *GitHubCopilotExecutor) normalizeModel(model string, body []byte) []byte {
-	baseModel := thinking.ParseSuffix(model).ModelName
-	bodyModel := gjson.GetBytes(body, "model").String()
-	if baseModel != bodyModel {
-		body, _ = sjson.SetBytes(body, "model", baseModel)
+	normalized := strings.TrimPrefix(model, "copilot-")
+	normalized = thinking.ParseSuffix(normalized).ModelName
+	if normalized != model {
+		body, _ = sjson.SetBytes(body, "model", normalized)
 	}
 	return body
 }
 
-func useGitHubCopilotResponsesEndpoint(sourceFormat sdktranslator.Format, model string) bool {
-	if sourceFormat.String() == "openai-response" {
-		return true
+func isGPT5Model(model string) bool {
+	normalized := strings.TrimPrefix(model, "copilot-")
+	return strings.HasPrefix(normalized, "gpt-5")
+}
+
+func isCopilotClaudeModel(model string) bool {
+	normalized := strings.TrimPrefix(model, "copilot-")
+	return strings.HasPrefix(normalized, "claude-")
+}
+
+func getEndpointPath(model string, format sdktranslator.Format) string {
+	if isCopilotClaudeFormat(format) {
+		return githubCopilotMessagesPath
+	}
+	if isGPT5Model(model) {
+		return githubCopilotResponsesPath
+	}
+	return githubCopilotChatPath
+}
+
+func isCopilotClaudeFormat(format sdktranslator.Format) bool {
+	return format == sdktranslator.FormatClaude
+}
+
+func normalizeCopilotClaudeThinking(model string, body []byte) []byte {
+	normalized := strings.ToLower(strings.TrimPrefix(model, "copilot-"))
+	supportsThinking := strings.Contains(normalized, "sonnet-4") ||
+		strings.Contains(normalized, "3-5-sonnet") ||
+		strings.Contains(normalized, "3.5-sonnet") ||
+		strings.Contains(normalized, "3-7-sonnet") ||
+		strings.Contains(normalized, "3.7-sonnet") ||
+		strings.Contains(normalized, "opus-4")
+	if !supportsThinking {
+		if updated, err := sjson.DeleteBytes(body, "thinking"); err == nil {
+			return updated
+		}
+		return body
+	}
+	thinkingVal := gjson.GetBytes(body, "thinking")
+	thinkingType := strings.ToLower(strings.TrimSpace(thinkingVal.Get("type").String()))
+	if thinkingType == "disabled" {
+		return body
+	}
+	maxTokens := gjson.GetBytes(body, "max_tokens")
+	maxVal := int(maxTokens.Int())
+	if maxVal <= 0 {
+		maxVal = 16000
+		body, _ = sjson.SetBytes(body, "max_tokens", maxVal)
+	}
+	budgetVal := int(thinkingVal.Get("budget_tokens").Int())
+	if budgetVal <= 0 {
+		budgetVal = 10000
+	}
+	if budgetVal < 1024 {
+		budgetVal = 1024
 	}
-	baseModel := strings.ToLower(thinking.ParseSuffix(model).ModelName)
-	return strings.Contains(baseModel, "codex")
+	if budgetVal > 128000 {
+		budgetVal = 128000
+	}
+	if budgetVal >= maxVal {
+		budgetVal = maxVal - 1
+	}
+	body, _ = sjson.SetBytes(body, "thinking.type", "enabled")
+	body, _ = sjson.SetBytes(body, "thinking.budget_tokens", budgetVal)
+	return body
 }
 
 // flattenAssistantContent converts assistant message content from array format
diff --git a/internal/runtime/executor/github_copilot_executor_test.go b/internal/runtime/executor/github_copilot_executor_test.go
--- a/internal/runtime/executor/github_copilot_executor_test.go
+++ b/internal/runtime/executor/github_copilot_executor_test.go
@@ -56,24 +56,29 @@
 	}
 }
 
-func TestUseGitHubCopilotResponsesEndpoint_OpenAIResponseSource(t *testing.T) {
+func TestIsGPT5Model(t *testing.T) {
 	t.Parallel()
-	if !useGitHubCopilotResponsesEndpoint(sdktranslator.FromString("openai-response"), "claude-3-5-sonnet") {
-		t.Fatal("expected openai-response source to use /responses")
+	if !isGPT5Model("gpt-5-codex") {
+		t.Fatal("expected gpt-5-codex to be GPT5 model")
 	}
-}
-
-func TestUseGitHubCopilotResponsesEndpoint_CodexModel(t *testing.T) {
-	t.Parallel()
-	if !useGitHubCopilotResponsesEndpoint(sdktranslator.FromString("openai"), "gpt-5-codex") {
-		t.Fatal("expected codex model to use /responses")
+	if !isGPT5Model("copilot-gpt-5-codex") {
+		t.Fatal("expected copilot-gpt-5-codex to be GPT5 model")
+	}
+	if isGPT5Model("claude-3-5-sonnet") {
+		t.Fatal("expected claude model not to be GPT5")
 	}
 }
 
-func TestUseGitHubCopilotResponsesEndpoint_DefaultChat(t *testing.T) {
+func TestIsCopilotClaudeModel(t *testing.T) {
 	t.Parallel()
-	if useGitHubCopilotResponsesEndpoint(sdktranslator.FromString("openai"), "claude-3-5-sonnet") {
-		t.Fatal("expected default openai source with non-codex model to use /chat/completions")
+	if !isCopilotClaudeModel("claude-3-5-sonnet") {
+		t.Fatal("expected claude-3-5-sonnet to be Claude model")
+	}
+	if !isCopilotClaudeModel("copilot-claude-sonnet-4") {
+		t.Fatal("expected copilot-claude-sonnet-4 to be Claude model")
+	}
+	if isCopilotClaudeModel("gpt-5-codex") {
+		t.Fatal("expected gpt-5 not to be Claude model")
 	}
 }
 
@@ -256,9 +261,9 @@
 	e := &GitHubCopilotExecutor{}
 	req, _ := http.NewRequest(http.MethodPost, "https://example.com", nil)
 	body := []byte(`{"messages":[{"role":"system","content":"sys"},{"role":"user","content":"hello"}]}`)
-	e.applyHeaders(req, "token", body)
-	if got := req.Header.Get("X-Initiator"); got != "user" {
-		t.Fatalf("X-Initiator = %q, want user", got)
+	e.applyHeaders(req, "token", sdktranslator.FormatOpenAI, body)
+	if got := req.Header.Get("X-Initiator"); got != "agent" {
+		t.Fatalf("X-Initiator = %q, want agent", got)
 	}
 }
 
@@ -268,7 +273,7 @@
 	req, _ := http.NewRequest(http.MethodPost, "https://example.com", nil)
 	// Claude Code typical flow: last message is user (tool result), but has assistant in history
 	body := []byte(`{"messages":[{"role":"user","content":"hello"},{"role":"assistant","content":"I will read the file"},{"role":"user","content":"tool result here"}]}`)
-	e.applyHeaders(req, "token", body)
+	e.applyHeaders(req, "token", sdktranslator.FormatOpenAI, body)
 	if got := req.Header.Get("X-Initiator"); got != "agent" {
 		t.Fatalf("X-Initiator = %q, want agent (assistant exists in messages)", got)
 	}
@@ -279,7 +284,7 @@
 	e := &GitHubCopilotExecutor{}
 	req, _ := http.NewRequest(http.MethodPost, "https://example.com", nil)
 	body := []byte(`{"messages":[{"role":"user","content":"hello"},{"role":"tool","content":"result"}]}`)
-	e.applyHeaders(req, "token", body)
+	e.applyHeaders(req, "token", sdktranslator.FormatOpenAI, body)
 	if got := req.Header.Get("X-Initiator"); got != "agent" {
 		t.Fatalf("X-Initiator = %q, want agent (tool role exists)", got)
 	}
@@ -291,9 +296,9 @@
 	t.Parallel()
 	e := &GitHubCopilotExecutor{}
 	req, _ := http.NewRequest(http.MethodPost, "https://example.com", nil)
-	e.applyHeaders(req, "token", nil)
-	if got := req.Header.Get("X-Github-Api-Version"); got != "2025-04-01" {
-		t.Fatalf("X-Github-Api-Version = %q, want 2025-04-01", got)
+	e.applyHeaders(req, "token", sdktranslator.FormatOpenAI, nil)
+	if got := req.Header.Get("X-Github-Api-Version"); got != "2025-10-01" {
+		t.Fatalf("X-Github-Api-Version = %q, want 2025-10-01", got)
 	}
 }
 
diff --git a/internal/translator/claude/openai/chat-completions/claude_openai_response.go b/internal/translator/claude/openai/chat-completions/claude_openai_response.go
--- a/internal/translator/claude/openai/chat-completions/claude_openai_response.go
+++ b/internal/translator/claude/openai/chat-completions/claude_openai_response.go
@@ -282,6 +282,15 @@
 		chunks = append(chunks, bytes.TrimSpace(line[5:]))
 	}
 
+	// If no SSE data lines found, check if this is a plain Claude JSON response
+	// (e.g. from Copilot's /v1/messages endpoint in non-streaming mode)
+	if len(chunks) == 0 {
+		root := gjson.ParseBytes(rawJSON)
+		if root.Get("type").String() == "message" {
+			return convertPlainClaudeResponseToOpenAI(root)
+		}
+	}
+
 	// Base OpenAI non-streaming response template
 	out := `{"id":"","object":"chat.completion","created":0,"model":"","choices":[{"index":0,"message":{"role":"assistant","content":""},"finish_reason":"stop"}],"usage":{"prompt_tokens":0,"completion_tokens":0,"total_tokens":0}}`
 
@@ -433,4 +442,76 @@
 	}
 
 	return out
+}
+
+// convertPlainClaudeResponseToOpenAI converts a plain (non-SSE) Claude Messages API
+// response into OpenAI chat completion format. This handles responses from endpoints
+// like Copilot's /v1/messages when stream=false.
+func convertPlainClaudeResponseToOpenAI(root gjson.Result) string {
+	out := `{"id":"","object":"chat.completion","created":0,"model":"","choices":[{"index":0,"message":{"role":"assistant","content":""},"finish_reason":"stop"}],"usage":{"prompt_tokens":0,"completion_tokens":0,"total_tokens":0}}`
+
+	out, _ = sjson.Set(out, "id", root.Get("id").String())
+	out, _ = sjson.Set(out, "created", time.Now().Unix())
+	out, _ = sjson.Set(out, "model", root.Get("model").String())
+
+	var contentParts []string
+	var reasoningParts []string
+	toolCallsCount := 0
+
+	content := root.Get("content")
+	if content.IsArray() {
+		for _, block := range content.Array() {
+			blockType := block.Get("type").String()
+			switch blockType {
+			case "text":
+				contentParts = append(contentParts, block.Get("text").String())
+			case "thinking":
+				reasoningParts = append(reasoningParts, block.Get("thinking").String())
+			case "tool_use":
+				idPath := fmt.Sprintf("choices.0.message.tool_calls.%d.id", toolCallsCount)
+				typePath := fmt.Sprintf("choices.0.message.tool_calls.%d.type", toolCallsCount)
+				namePath := fmt.Sprintf("choices.0.message.tool_calls.%d.function.name", toolCallsCount)
+				argumentsPath := fmt.Sprintf("choices.0.message.tool_calls.%d.function.arguments", toolCallsCount)
+				out, _ = sjson.Set(out, idPath, block.Get("id").String())
+				out, _ = sjson.Set(out, typePath, "function")
+				out, _ = sjson.Set(out, namePath, block.Get("name").String())
+				args := block.Get("input").Raw
+				if args == "" {
+					args = "{}"
+				}
+				out, _ = sjson.Set(out, argumentsPath, args)
+				toolCallsCount++
+			}
+		}
+	}
+
+	out, _ = sjson.Set(out, "choices.0.message.content", strings.Join(contentParts, ""))
+
+	if len(reasoningParts) > 0 {
+		out, _ = sjson.Set(out, "choices.0.message.reasoning", strings.Join(reasoningParts, ""))
+	}
+
+	stopReason := root.Get("stop_reason").String()
+	if toolCallsCount > 0 {
+		out, _ = sjson.Set(out, "choices.0.finish_reason", "tool_calls")
+	} else {
+		out, _ = sjson.Set(out, "choices.0.finish_reason", mapAnthropicStopReasonToOpenAI(stopReason))
+	}
+
+	// Map usage
+	usage := root.Get("usage")
+	if usage.Exists() {
+		inputTokens := usage.Get("input_tokens").Int()
+		outputTokens := usage.Get("output_tokens").Int()
+		cacheReadInputTokens := usage.Get("cache_read_input_tokens").Int()
+		cacheCreationInputTokens := usage.Get("cache_creation_input_tokens").Int()
+		out, _ = sjson.Set(out, "usage.prompt_tokens", inputTokens+cacheCreationInputTokens)
+		out, _ = sjson.Set(out, "usage.completion_tokens", outputTokens)
+		out, _ = sjson.Set(out, "usage.total_tokens", inputTokens+outputTokens)
+		if cacheReadInputTokens > 0 {
+			out, _ = sjson.Set(out, "usage.prompt_tokens_details.cached_tokens", cacheReadInputTokens)
+		}
+	}
+
+	return out
 }
